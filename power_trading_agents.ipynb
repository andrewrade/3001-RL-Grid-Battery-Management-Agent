{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global _debug\n",
    "_debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gym_power_trading.envs import PowerTradingEnv\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callback for logging rewards across parallel environments during Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecRewardLogger(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(VecRewardLogger, self).__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.episode_rewards = None\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        # Initialize on training start to get the number of environments\n",
    "        self.episode_rewards = np.zeros(self.model.env.num_envs)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\" This method is called after each step \"\"\"\n",
    "        # Update rewards for all environments\n",
    "        self.episode_rewards += np.array(self.locals['rewards'])\n",
    "       \n",
    "        # Check if any episode is done\n",
    "        if np.any(self.locals['dones']):\n",
    "            avg_episode_reward = np.mean(self.episode_rewards)\n",
    "            self.rewards.append(avg_episode_reward)\n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                print(\"Logged rewards for completed episodes:\", [self.rewards[-1]])\n",
    "        return True  # Always return True to continue training\n",
    "\n",
    "logger = VecRewardLogger(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RT_LMP</th>\n",
       "      <th>DA_LMP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARKET_DAY</th>\n",
       "      <th>NODE</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>HourEnding</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-10-01</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">AEP.PSGC1.AMP</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">Gennode</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">LMP</th>\n",
       "      <th>1</th>\n",
       "      <td>10.02</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.51</td>\n",
       "      <td>13.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.79</td>\n",
       "      <td>12.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.53</td>\n",
       "      <td>13.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.43</td>\n",
       "      <td>14.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   RT_LMP  DA_LMP\n",
       "MARKET_DAY NODE          TYPE    VALUE HourEnding                \n",
       "2020-10-01 AEP.PSGC1.AMP Gennode LMP   1            10.02   14.33\n",
       "                                       2            12.51   13.09\n",
       "                                       3            12.79   12.96\n",
       "                                       4            12.53   13.62\n",
       "                                       5            13.43   14.49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"data\\AEP_PSGC1_AMP_dart_Oct20.h5\"\n",
    "path = Path(file_name).resolve()\n",
    "path_string = str(path)\n",
    "df = pd.read_hdf(path_string)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent with Basic Features \n",
    "\n",
    "### (4-hr Observation Window, 552,000 training iterations) \n",
    "\n",
    "Environment Observation Comprised of:\n",
    "-  Current Price (\\$/Mwh) / DA Price \\($/Mwh)\n",
    "-  Price Difference (from last tick) (%)\n",
    "-  Battery State of Charge (%)\n",
    "-  Battery Avg Energy Price (\\$/Mwh) / Battery Avg Energy Price Window-period Rolling Avg \\($/Mwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Rewards: \n",
    "\n",
    "    - Log Return of profitable discharges: Revenue / Power Sold * Stored Power Cost Basis\n",
    "    - Log of revenue when selling power with negative cost basis: ln(revenue + |Stored Power Cost Basis|)\n",
    "\n",
    "Penalty:\n",
    "\n",
    "    - Overcharging (-1)\n",
    "    - Discharging When Empty (-1)\n",
    "    - Losing Money (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew deur\\documents\\nyu\\ds-ga 3001 reinforcement learning\\3001-rl-grid-battery-management-agent\\gym-power-trading\\gym_power_trading\\envs\\power_trading.py:190: RuntimeWarning: divide by zero encountered in divide\n",
      "  pct_diff = np.insert(np.where(prices[:-1] != 0, diff / prices[:-1], 0), 0, 0) # Change from price 1-tick ago\n",
      "c:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.set_frame_bound to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.set_frame_bound` for environment variables or `env.get_wrapper_attr('set_frame_bound')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged rewards for completed episodes: [-34.83367958152667]\n",
      "Logged rewards for completed episodes: [-73.3456419975264]\n",
      "Logged rewards for completed episodes: [-115.74419277027482]\n",
      "Logged rewards for completed episodes: [-152.02488288287714]\n",
      "Logged rewards for completed episodes: [-192.00224183771934]\n",
      "Logged rewards for completed episodes: [-232.14587643595587]\n",
      "Logged rewards for completed episodes: [-270.8087983328005]\n",
      "Logged rewards for completed episodes: [-313.03195494068495]\n",
      "Logged rewards for completed episodes: [-26.479669201813522]\n",
      "Logged rewards for completed episodes: [-51.930359634701745]\n",
      "Logged rewards for completed episodes: [-76.75146941393905]\n",
      "Logged rewards for completed episodes: [-108.81105590514198]\n",
      "Logged rewards for completed episodes: [-129.15030926572217]\n",
      "Logged rewards for completed episodes: [-157.4098506650771]\n",
      "Logged rewards for completed episodes: [-184.92014754691627]\n",
      "Logged rewards for completed episodes: [-212.42382454589824]\n",
      "Logged rewards for completed episodes: [-13.386039129462421]\n",
      "Logged rewards for completed episodes: [-27.5352303881197]\n",
      "Logged rewards for completed episodes: [-50.427551508111655]\n",
      "Logged rewards for completed episodes: [-68.85187622673578]\n",
      "Logged rewards for completed episodes: [-84.02768160296722]\n",
      "Logged rewards for completed episodes: [-108.32788412999247]\n",
      "Logged rewards for completed episodes: [-122.02045426039143]\n",
      "Logged rewards for completed episodes: [-143.21479771349]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:23\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Andrew Deur\\anaconda3\\envs\\rl_project\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\users\\andrew deur\\documents\\nyu\\ds-ga 3001 reinforcement learning\\3001-rl-grid-battery-management-agent\\gym-power-trading\\gym_power_trading\\envs\\power_trading.py:109\u001b[0m, in \u001b[0;36mPowerTradingEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    106\u001b[0m trade \u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m!=\u001b[39m Actions\u001b[38;5;241m.\u001b[39mHold\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;66;03m# Trade = True if action is not hold\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Calculate reward & profit, update totals\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m step_reward, power_traded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward(action)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step_reward\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_profit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_profit(power_traded, action)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "window = 4 # hours\n",
    "total_timesteps = 552_000\n",
    "train_window = 240 # hrs \n",
    "n_envs = 4\n",
    "validation_size = 3000\n",
    "batch_timesteps = train_window * 1 * n_envs # 1 batch of train_window x window observations for each environment \n",
    "\n",
    "# Produce Observations across 16 parallel environments \n",
    "venv = make_vec_env(lambda: PowerTradingEnv(df=df, window_size=window, frame_bound=(0, train_window)), n_envs=n_envs) \n",
    "PPO_power_basic = PPO('MlpPolicy', venv, device='cpu')\n",
    "\n",
    "# Walk forward during training to simulate real environment + reduce agent memorization\n",
    "num_hours = (df.shape[0] - validation_size)\n",
    "num_eps = num_hours // train_window\n",
    "num_epochs = total_timesteps // (batch_timesteps * num_eps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_eps):\n",
    "        start_day = i * train_window\n",
    "        end_day = start_day + train_window \n",
    "        venv.env_method('set_frame_bound', start_day, end_day) # Set venvs frame bounds\n",
    "        # Learn on currrent minibatch\n",
    "        PPO_power_basic.learn(total_timesteps=batch_timesteps, callback=logger, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Reward Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = np.arange(1, len(logger.rewards) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(num_eps, logger.rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('PPO Agent (Basic Features) 552,000 Iterations: Episode Reward Progression')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Agent Trades in Held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 3\n",
    "last_index = len(df) - 10\n",
    "start_index = last_index - validation_size\n",
    "env = PowerTradingEnv(df=df, window_size=window, frame_bound=(start_index, last_index))\n",
    "obs = env.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "for i in range(last_index - start_index):\n",
    "    action, states = PPO_power_basic.predict(obs, )\n",
    "    obs, rewards, term, trunc, info = env.step(action)\n",
    "    if term or trunc:\n",
    "        history = env.history\n",
    "        env.render_all(title=\"PPO Agent (Basic) Positions on Test Set @ 552,000 Training Steps\", xlim=(1100, 1500))\n",
    "        env.reset()\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Statistics over 100 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, reward_std = evaluate_policy(PPO_power_basic, env, n_eval_episodes=100)\n",
    "print(f\"Mean Reward:{ mean_reward:,.2f}\\nReward std: {reward_std:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Evolution on Held Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['total_reward'])\n",
    "plt.title(\"PPO (Basic) Reward on Test Set @ 552,000 Training Steps\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Battery Charge Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['battery_charge'])\n",
    "plt.title(\"PPO (Basic) Battery Charge Management (552,000 Training Steps)\")\n",
    "plt.ylabel(\"Mwh\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit Evolution on Held Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['total_profit'])\n",
    "plt.title(\"PPO (Basic) Profit on Test Set @ 552,000 Training Steps\")\n",
    "plt.ylabel(\"$\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_power_basic.save('PPO_power_basic_552k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent with Advanced Features\n",
    "\n",
    "### (4-hr Observation Window, 552,000 training iterations) \n",
    "\n",
    "Environment Observation Comprised of:\n",
    "-  Current Price (\\$/Mwh) \n",
    "-  DA Price \\($/Mwh)\n",
    "-  Battery State of Charge (%)\n",
    "-  Battery Avg Energy Price (\\$/Mwh) / Battery Avg Energy Price Window-period Rolling Avg \\($/Mwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards: \n",
    "\n",
    "    - Log Return of profitable discharges: Revenue / Power Sold * Stored Power Cost Basis\n",
    "    - Log of revenue when selling power with negative cost basis: ln(revenue + |Stored Power Cost Basis|)\n",
    "\n",
    "Penalty:\n",
    "\n",
    "    - Overcharging (-1)\n",
    "    - Discharging When Empty (-1)\n",
    "    - Losing Money (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "window = 4 # hours\n",
    "total_timesteps = 552_000\n",
    "train_window = 240 # hrs \n",
    "n_envs = 4\n",
    "validation_size = 3000\n",
    "batch_timesteps = train_window * 1 * n_envs # 1 batch of train_window x window observations for each environment \n",
    "\n",
    "# Produce Observations across 16 parallel environments \n",
    "venv2 = make_vec_env(lambda: PowerTradingEnv(df=df, window_size=window, frame_bound=(0, train_window)), n_envs=n_envs) \n",
    "PPO_power_adv = PPO('MlpPolicy', venv, device='cpu')\n",
    "logger_adv = VecRewardLogger(verbose=1)\n",
    "\n",
    "# Walk forward during training to simulate real environment + reduce agent memorization\n",
    "num_hours = (df.shape[0] - validation_size)\n",
    "num_eps = num_hours // train_window\n",
    "num_epochs = total_timesteps // (batch_timesteps * num_eps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_eps):\n",
    "        start_day = i * train_window\n",
    "        end_day = start_day + train_window \n",
    "        venv2.env_method('set_frame_bound', start_day, end_day) # Set venvs frame bounds\n",
    "        # Learn on currrent minibatch\n",
    "        PPO_power_adv.learn(total_timesteps=batch_timesteps, callback=logger_adv, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Reward Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps = np.arange(1, len(logger_adv.rewards) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(num_eps, logger_adv.rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('PPO Agent (Advanced Features) 552,000 Iterations: Episode Reward Progression')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Agent Trades in Held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = len(df) - 10\n",
    "start_index = last_index - 3000\n",
    "env2 = PowerTradingEnv(df=df, window_size=window, frame_bound=(start_index, last_index))\n",
    "obs = env2.reset()\n",
    "obs = obs[0]\n",
    "\n",
    "for i in range(last_index - start_index):\n",
    "    action, states = PPO_power_adv.predict(obs)\n",
    "    obs, rewards, term, trunc, info = env2.step(action)\n",
    "    if term or trunc:\n",
    "        history = env2.history\n",
    "        pos = env2.render_all(title=\"PPO Agent (Advanced) Positions on Test Set @ 552,000 Training Steps\", xlim=(1100, 1500))\n",
    "        env2.reset()\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Statistics over 100 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, reward_std = evaluate_policy(PPO_power_adv, env2, n_eval_episodes=100)\n",
    "print(f\"Mean Reward:{ mean_reward:,.2f}\\nReward std: {reward_std:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Evolution on Held Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['total_reward'])\n",
    "plt.title(\"PPO Agent (Advanced) Reward on Test Set @ 552,000 Training Steps\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Battery Charge Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PPO 10,000 Steps on Sinusoid\n",
    "plt.plot(history['battery_charge'])\n",
    "plt.title(\"PPO Agent Battery Charge (Advanced) Reward on Test Set @ 552,000 Training Steps\")\n",
    "plt.ylabel(\"Mwh\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profit Evolution on Held Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['total_profit'])\n",
    "plt.title(\"PPO Agent Profit (Advanced) Reward on Test Set @ 552,000 Training Steps\")\n",
    "plt.ylabel(\"$\")\n",
    "plt.xlabel(\"Number of Timesteps (1hr)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_power_adv.save('PPO_power_advanced_552k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
